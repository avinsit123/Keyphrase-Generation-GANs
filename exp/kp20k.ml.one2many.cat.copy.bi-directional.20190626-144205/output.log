06/26/2019 14:42:05 [INFO] train: Parameters:
06/26/2019 14:42:05 [INFO] train: vocab_size    :    50002
06/26/2019 14:42:05 [INFO] train: max_unk_words    :    1000
06/26/2019 14:42:05 [INFO] train: words_min_frequency    :    0
06/26/2019 14:42:05 [INFO] train: dynamic_dict    :    True
06/26/2019 14:42:05 [INFO] train: word_vec_size    :    100
06/26/2019 14:42:05 [INFO] train: share_embeddings    :    True
06/26/2019 14:42:05 [INFO] train: use_target_encoder    :    False
06/26/2019 14:42:05 [INFO] train: encoder_type    :    rnn
06/26/2019 14:42:05 [INFO] train: decoder_type    :    rnn
06/26/2019 14:42:05 [INFO] train: enc_layers    :    1
06/26/2019 14:42:05 [INFO] train: dec_layers    :    1
06/26/2019 14:42:05 [INFO] train: encoder_size    :    150
06/26/2019 14:42:05 [INFO] train: decoder_size    :    300
06/26/2019 14:42:05 [INFO] train: target_encoder_size    :    64
06/26/2019 14:42:05 [INFO] train: source_representation_queue_size    :    128
06/26/2019 14:42:05 [INFO] train: source_representation_sample_size    :    32
06/26/2019 14:42:05 [INFO] train: dropout    :    0.1
06/26/2019 14:42:05 [INFO] train: bidirectional    :    True
06/26/2019 14:42:05 [INFO] train: bridge    :    copy
06/26/2019 14:42:05 [INFO] train: attn_mode    :    concat
06/26/2019 14:42:05 [INFO] train: copy_attention    :    True
06/26/2019 14:42:05 [INFO] train: coverage_attn    :    False
06/26/2019 14:42:05 [INFO] train: review_attn    :    False
06/26/2019 14:42:05 [INFO] train: lambda_coverage    :    1
06/26/2019 14:42:05 [INFO] train: coverage_loss    :    False
06/26/2019 14:42:05 [INFO] train: orthogonal_loss    :    False
06/26/2019 14:42:05 [INFO] train: lambda_orthogonal    :    0.03
06/26/2019 14:42:05 [INFO] train: lambda_target_encoder    :    0.03
06/26/2019 14:42:05 [INFO] train: separate_present_absent    :    False
06/26/2019 14:42:05 [INFO] train: manager_mode    :    1
06/26/2019 14:42:05 [INFO] train: goal_vector_size    :    16
06/26/2019 14:42:05 [INFO] train: goal_vector_mode    :    0
06/26/2019 14:42:05 [INFO] train: title_guided    :    False
06/26/2019 14:42:05 [INFO] train: data    :    data/kp20k_tg_sorted/
06/26/2019 14:42:05 [INFO] train: vocab    :    data/kp20k_tg_sorted/
06/26/2019 14:42:05 [INFO] train: custom_data_filename_suffix    :    False
06/26/2019 14:42:05 [INFO] train: custom_vocab_filename_suffix    :    False
06/26/2019 14:42:05 [INFO] train: vocab_filename_suffix    :    
06/26/2019 14:42:05 [INFO] train: data_filename_suffix    :    
06/26/2019 14:42:05 [INFO] train: save_model    :    model
06/26/2019 14:42:05 [INFO] train: train_from    :    
06/26/2019 14:42:05 [INFO] train: gpuid    :    0
06/26/2019 14:42:05 [INFO] train: seed    :    9527
06/26/2019 14:42:05 [INFO] train: epochs    :    20
06/26/2019 14:42:05 [INFO] train: start_epoch    :    1
06/26/2019 14:42:05 [INFO] train: param_init    :    0.1
06/26/2019 14:42:05 [INFO] train: pre_word_vecs_enc    :    None
06/26/2019 14:42:05 [INFO] train: pre_word_vecs_dec    :    None
06/26/2019 14:42:05 [INFO] train: fix_word_vecs_enc    :    False
06/26/2019 14:42:05 [INFO] train: fix_word_vecs_dec    :    False
06/26/2019 14:42:05 [INFO] train: batch_size    :    12
06/26/2019 14:42:05 [INFO] train: batch_workers    :    4
06/26/2019 14:42:05 [INFO] train: optim    :    adam
06/26/2019 14:42:05 [INFO] train: max_grad_norm    :    1
06/26/2019 14:42:05 [INFO] train: truncated_decoder    :    0
06/26/2019 14:42:05 [INFO] train: loss_normalization    :    tokens
06/26/2019 14:42:05 [INFO] train: train_ml    :    True
06/26/2019 14:42:05 [INFO] train: train_rl    :    False
06/26/2019 14:42:05 [INFO] train: max_sample_length    :    6
06/26/2019 14:42:05 [INFO] train: max_length    :    6
06/26/2019 14:42:05 [INFO] train: topk    :    M
06/26/2019 14:42:05 [INFO] train: reward_type    :    0
06/26/2019 14:42:05 [INFO] train: match_type    :    exact
06/26/2019 14:42:05 [INFO] train: pretrained_model    :    
06/26/2019 14:42:05 [INFO] train: reward_shaping    :    False
06/26/2019 14:42:05 [INFO] train: baseline    :    self
06/26/2019 14:42:05 [INFO] train: mc_rollouts    :    False
06/26/2019 14:42:05 [INFO] train: num_rollouts    :    3
06/26/2019 14:42:05 [INFO] train: delimiter_type    :    0
06/26/2019 14:42:05 [INFO] train: one2many    :    True
06/26/2019 14:42:05 [INFO] train: one2many_mode    :    1
06/26/2019 14:42:05 [INFO] train: num_predictions    :    1
06/26/2019 14:42:05 [INFO] train: init_perturb_std    :    0
06/26/2019 14:42:05 [INFO] train: final_perturb_std    :    0
06/26/2019 14:42:05 [INFO] train: perturb_decay_mode    :    1
06/26/2019 14:42:05 [INFO] train: perturb_decay_factor    :    0.0001
06/26/2019 14:42:05 [INFO] train: perturb_baseline    :    False
06/26/2019 14:42:05 [INFO] train: regularization_type    :    0
06/26/2019 14:42:05 [INFO] train: regularization_factor    :    0.0
06/26/2019 14:42:05 [INFO] train: replace_unk    :    False
06/26/2019 14:42:05 [INFO] train: remove_src_eos    :    False
06/26/2019 14:42:05 [INFO] train: must_teacher_forcing    :    False
06/26/2019 14:42:05 [INFO] train: teacher_forcing_ratio    :    0
06/26/2019 14:42:05 [INFO] train: scheduled_sampling    :    False
06/26/2019 14:42:05 [INFO] train: scheduled_sampling_batches    :    10000
06/26/2019 14:42:05 [INFO] train: learning_rate    :    0.001
06/26/2019 14:42:05 [INFO] train: learning_rate_rl    :    5e-05
06/26/2019 14:42:05 [INFO] train: learning_rate_decay_rl    :    False
06/26/2019 14:42:05 [INFO] train: learning_rate_decay    :    0.5
06/26/2019 14:42:05 [INFO] train: start_decay_at    :    8
06/26/2019 14:42:05 [INFO] train: start_checkpoint_at    :    2
06/26/2019 14:42:05 [INFO] train: decay_method    :    
06/26/2019 14:42:05 [INFO] train: warmup_steps    :    4000
06/26/2019 14:42:05 [INFO] train: checkpoint_interval    :    4000
06/26/2019 14:42:05 [INFO] train: disable_early_stop_rl    :    False
06/26/2019 14:42:05 [INFO] train: early_stop_tolerance    :    4
06/26/2019 14:42:05 [INFO] train: timemark    :    20190626-144205
06/26/2019 14:42:05 [INFO] train: report_every    :    10
06/26/2019 14:42:05 [INFO] train: exp    :    kp20k.ml.one2many.cat.copy.bi-directional
06/26/2019 14:42:05 [INFO] train: exp_path    :    exp/kp20k.ml.one2many.cat.copy.bi-directional.20190626-144205
06/26/2019 14:42:05 [INFO] train: model_path    :    model/kp20k.ml.one2many.cat.copy.bi-directional.20190626-144205
06/26/2019 14:42:05 [INFO] train: delimiter_word    :    <sep>
06/26/2019 14:42:05 [INFO] train: input_feeding    :    False
06/26/2019 14:42:05 [INFO] train: copy_input_feeding    :    False
06/26/2019 14:42:05 [INFO] train: device    :    cuda:0
06/26/2019 14:42:05 [INFO] data_loader: Loading vocab from disk: data/kp20k_tg_sorted/
06/26/2019 14:42:06 [INFO] data_loader: #(vocab)=344733
06/26/2019 14:42:06 [INFO] data_loader: #(vocab used)=50002
06/26/2019 14:42:06 [INFO] data_loader: Loading train and validate data from 'data/kp20k_tg_sorted/'
06/26/2019 14:43:55 [INFO] data_loader: #(train data size: #(batch)=40951
06/26/2019 14:43:56 [INFO] data_loader: #(valid data size: #(batch)=1667
06/26/2019 14:43:58 [INFO] train: Time for loading the data: 112.3
06/26/2019 14:43:58 [INFO] train: ======================  Model Parameters  =========================
06/26/2019 14:43:58 [INFO] train: Training a seq2seq model with copy mechanism
06/26/2019 14:44:00 [ERROR] train: message
Traceback (most recent call last):
  File "train.py", line 161, in main
    model = init_model(opt)
  File "train.py", line 151, in init_model
    return model.to(opt.device)
  File "/dev/shm/avinash/mypython/lib/python3.6/site-packages/torch/nn/modules/module.py", line 386, in to
    return self._apply(convert)
  File "/dev/shm/avinash/mypython/lib/python3.6/site-packages/torch/nn/modules/module.py", line 193, in _apply
    module._apply(fn)
  File "/dev/shm/avinash/mypython/lib/python3.6/site-packages/torch/nn/modules/module.py", line 193, in _apply
    module._apply(fn)
  File "/dev/shm/avinash/mypython/lib/python3.6/site-packages/torch/nn/modules/module.py", line 199, in _apply
    param.data = fn(param.data)
  File "/dev/shm/avinash/mypython/lib/python3.6/site-packages/torch/nn/modules/module.py", line 384, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: out of memory
